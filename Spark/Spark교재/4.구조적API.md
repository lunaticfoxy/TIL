
#### 4.1 DataFrame과 Dataset
- DataFrame과 Dataset은 모두 잘 정의된 로우와 컬럼을 가지는 분산 테이블 컬렉션
  - 분산 테이블: 익스큐터마다 나눠서 가지고 저장된다 - 묶으려면 collect 필요
- DB의 테이블, 뷰와 1:1 매핑되는 개념


#### 4.2 스키마
- 컬럼명과 데이터 타입 정의


#### 4.3 스파크의 구조적 데이터 타입 개요
- 스파크는 작업 수행 계획을 위해 "카탈리스트" 엔진 사용
- 스파크에서 다음 연산이 가능한 이유는 스파크의 데이터타입이 별도로 존재하기 때문
```scala
val df = spark.range(500).toDF("number")
df.select(df.col("number") + 10)
```
```python
df = spark.range(500).toDF("number")
df.select(df["number"]) + 10)
```

##### 4.3.1 DataFrame과 Dataset 비교
- Dataset: 타입형
  - 데이터 타입을 "컴파일 타임"에 체크
  - JVM 기반의 Java와 Scala에서만 사용 가능
- DataFrame: 비타입형
  - 엄밀히는 타입은 존재하나 "런타임"에 체크
  - 모든 언어에서 사용 가능
    - Python, R 등
  - 사실상 Row 타입으로 구성된 Dataset이라고 보면 됨
    - Row타입으로 한번 감싸서 Dataset의 컴파일 타임 데이터 체크를 통과
  - 일반적으로는 Dataset보다 DataFrame을 사용

##### 4.3.2 컬럼
- 단순 데이터 타입: 정수, 문자, 실수 등
- 복합 데이터 타입: 배열, 맵 등
- null값: 값이 없다는 의미로 활용

##### 4.3.3 로우
- DataFrame의 레코드는 Row타입으로 구성
- 한 라인의 데이터를 포함

##### 4.3.4 스파크 데이터 타입
- 스파크 데이터 타입을 스칼라에서 사용하려면 org.apache.spark.sql.type._ 를 임포트
- 주요 데이터 타입
  - ByteType
    - python: int, long
    - scala: Byte
  - IntegerType
    - python: int, long
    - scala: Int
  - LongType : Integer와 다르므로 주의해야함
    - python: long
    - scala Long
  - DoubleType
    - python: float
    - scala: Double
  - StringType
    - python: string
    - scala: String
  - ArrayType
    - python: list, tuple, array
    - scala: scala.collection.Seq
  - StructType
    - python: list, tuple
    - scala: org.apache.spark.sql.Row
  - StructField: 각 언어의 타입을 사용하겠다는 표시


#### 4.4 구조적 API의 실행과정
- 구조적 API가 사용자의 코드를 실행 코드로 변환하는 과정
  1. DataFrame/Dataset/SQL을 통해 코드 작성
  2. 코드 오류를 체크하고 "논리적 실행 계획" 으로 변경
  3. "논리적 실행 계획"을 "물리적 실행 계획" 으로 변경하고 최적화
    - RDD 레벨로 변환됨
  4. "물리적 실행 계획" 처리

##### 4.4.1 논리적 실행 계획
- 추상적 트랜스포메이션만 표현
  - 드라이버나 익스큐터의 정보 미고려
- 사용자 표현식 최적화
- 컬럼, 테이블에 대한 검증

##### 4.4.2 물리적 실행 계획
- 논리적 실행 계획을 클러스터 환경에서 실행하는 방법 정의
- 다양한 물리적 실행 계획으로 변환한 뒤 가장 비용이 적게 드는 계획 선택
- RDD로 변환되어 수행
- 컴파일러의 동작과 유사

##### 4.4.3 실행
- 런타임에 테스트, 스테이지 관리를 위한 자바 바이트코드를 생성
  - 추가적이 최적화에 사용 (skipped 뜨는 그거)
