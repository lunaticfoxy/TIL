- 집계함수
  - 키나 그룹을 지정하고 하나 이상의 컬럼을 변환하는 방법 지정
  - 여러 입력값에 대해 그룹별로 결과 생성
  - 합, 곱, 카운팅, 평균 등
  
- 그룹화 데이터 타입 생성 방법
  - select 구문에서 직접 그룹화
  - group by: 가장 기본
  - window
  - grouping set: 서로 다른 레벨의 값 집계
  - roll up: 계층적인 그룹화
  - cube: 컬럼 조합에 대한 요약

- 그룹화 결과는 RelationalGroupedDataset 형태로 저장


####  7.1 집계 함수
- 모든 집계는 함수 사용
  - 예외는 있으나 무시할만한 수준이므로 패스
  - org.apache.spark.sql.functions 내에 존재
- 데이터 프레임레벨과 그룹 레벨에서 모두 사용 가능
  
##### 7.1.1 count
- 액션이 아닌 트랜스포메이션
  - 액션: 바로 결과를 출력한다 - 특정 값의 출력
  - 트랜스포메이션: 지연해서 결과를 출력한다 - 데이터의 변환
- 사용 방법
  - count(필드명) : 해당 필드값이 null 이 아닌 카운트 수 출력
  - count(1) : 필드값 상관없이 전체 카운트 출력

##### 7.1.2 countDistinct
- count와 동일하게 쓰지만 distinct된 수 출력

##### 7.1.3 approx_count_distinct
- 정확한 distinct count가 아닌 근사치 출력
- 사용방법
  - approx_count_distinct(필드명, 최대 추정 오류율)
  - 최대 추정 오류율에는 비율 넣으면 됨
    
##### 7.1.4 fisrt, last
- 맨 앞이나 마지막 값 출력
- 실제 저장된 row를 기반으로 동작

##### 7.1.5, 6, 7, 8 min, max, sum, sumDistinct, avg, mean
- 최소값, 최대값, 합, 중복 제거 합, 평균
- 다음과 같은 형태로도 집계 함수 사용 가능
```scala
df.select(
    count("quantity").alias("total_cnt"),         // 직접 집계 함수 사용
    sum("quantity").alias("total_sum"),
    avg("quantity").alias("total_avg"),
    expr("mean(quantity)").alias("total_mean"))   // expr로 집계 함수 사용 명령을 넘겨줌
  .selectExpr(
    "total_sum/total_cnt",                        // 연산이 있다는걸 seletExpr로 알려주고 연산 수행
    "total_avg",
    "total_mean")
```


##### 7.1.9 분산과 표준편차
- 모 분산, 표준편차: var_pop, stddev_pop
- 표본 분산, 표준편차: var_samp, stddev_samp

##### 7.1.10 비대칭도와 첨도
- 비대칭도 (skewnewss), 첨도 (kurtosis) 모두 데이터의 변곡점을 측정하는 방식
  - 비대칭도: 데이터 평균의 비대칭 정도 측정
  - 첨도: 데이터의 끝 부분 측정

##### 7.1.11 공분산과 상관관계
- 상관관계: corr(필드명1, 필드명2)
- 모공분산: covar_pop(필드명1, 필드명2)
- 표본공분산: covar_samp(필드명1, 필드명2)

##### 7.1.12 복합 데이터 타입의 집계
- 어떤 필드에 어떤 데이터가 있는지 확인해서 리스트로 변환
- collect_set(필드명): 중복을 제거해서 리스트로 변환
- collect_list(필드명): 중복을 허용해서 리스트로 변환
```scala
df.agg(collect_set("Country"), collect_list("Country")).show()
// collect_set(Country)           | collect_list(Country)
// [Portugal, Italy, France, ...] | [Portugal, Italy, Italy, ...]
```


#### 7.2 그룹화
- 데이터 그룹 단위의 집계를 수행하는 방법
- 두 단계로 나누어짐
  - 1. 그룹화 단계
    - 데이터를 기준에 따라 그룹화
    - RelationalGroupedDataset 형태로 리턴
  - 2. 집계 단계
    - 그룹화된 데이터에 집계함수를 적용
    - DataFrame 형태
```scala
df.groupBy("InvoiceNo", "CustomerId").count().show()
```

##### 7.2.1 표현식을 이용한 그룹화
- agg를 사용해서 표현식으로 그룹 데이터 집계 가능
```scala
df.groupBy("InvoiceNo").agg(
    count("Quantity").alias("quan"),
    expr("count(Quantity)"))
.show() // 둘다 같은값 출력
```

##### 7.2.2 맵을 이용한 그룹화
- 컬럼을 키, 수행할 집계 함수의 이름을 값으로 하는 map을 사용해서 그룹 트랜스포메이션 정의 가능
- 다양한 연산시나 연산 재활용시 용이
```scala
df.groupBy("InvoiceNo").agg("Quantity" -> "avg", "Quantity" -> "stddev_pop").show()

val temp = Map[String, String]("Quantity" -> "avg", "Quantity" -> "stddev_pop")  // 해보진 않았음... 대충 이런 의미
df.groupBy("InvoiceNo").agg(temp).show()
```


#### 7.3 윈도우 함수
- 데이터의 특정 "윈도우"에 대해 집계연산
- groupBy와의 차이점
  - groupBy
    - 그룹 내에 있는 데이터의 "통계"를 구한다
    - 리턴값은 그룹마다 하나씩이다
    - 범위는 항상 키로 나뉜 그룹 전체이다
    - 주 사용 범위: 전체 합, 전체 평균, Top 1, Bottom 1 등
  - window
    - 그룹 내에 있는 데이터끼리 "연산한 결과"를 가져온다
      - 물론 groupBy기능도 구성 가능
    - 리턴값은 모든 로우에 포함된다
    - 윈도우의 범위는 키로 나뉜 그룹 중 기준 row를 i라 할때 [i-x, i+y] 범위 내에 있는 값들로 구성된다
    - 주 사용 범위: Top N, moving average, row diff, 누적 합 등
- 사용방법
  - Window의 특성(Window Spec)을 먼저 정의한다
    - 파티션, 정렬, 데이터 범위 등
  - DataFrame.withColum("필드명", 집계함수.over(WindowSpec)) 형태로 호출한다
  - 이후 "필드명" 필드에 (집계함수 + WindowSpec)의 조합으로 연산된 값이 저장된다
```scala
import org.apache.spark.sql._
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window

val spark = SparkSession.builder().appName("sparksql").master("local").getOrCreate()
import spark.implicits._

val df =
  Seq(
    ("curycu", "2018-01-01", 10),
    ("curycu", "2018-01-07", 3),
    ("curycu", "2018-01-12", 7),
    ("curycu", "2018-01-15", 2),
    ("tester", "2018-01-01", 1),
    ("tester", "2018-01-11", 3),
    ("tester", "2018-01-18", 10))
    .toDF("id", "date", "amount")



//// Top 2 출력
val w = Window.partitionBy("id").orderBy("date")   // id를 키로 가지는 그룹에 그룹내에는 date를 기준으로 정렬하는 형태의 윈도우 정의

df.withColumn("rank", rank().over(w))   // rank 필드에는 w 에서 정의한 연산 (같은 id끼리 묶은 뒤 date 기준 정렬)을 적용하고 rank를 수행한다
  .filter(expr("rank <= 2"))
  .select("id", "date", "rank")
  .show()                               // rank가 1, 2인 ("tester", "2018-01-01", 1), ("tester", "2018-01-11", 2), 
                                        // ("curycu", "2018-01-01", 1), ("curycu", "2018-01-07", 2) 가 출력된다.


//// 부분합 계산
val wms3 = Window.partitionBy("id").orderBy("date").rowsBetween(-1, 1)  // id를 키로 가지는 그룹에 그룹내에는 date를 기준으로 정렬하며
                                                                        // 기준부터 위1, 아래1 까지 범위 3인 윈도우 정의

df.withColumn("ms3_amount", sum($"amount").over(wms3))  // ms3_amount 필드에는 wms3 에서 정의한 연산을 적용하고 3개씩 나누어 합을 구한다
  .select("id", "amount", "ms3_amount")
  .show()
// +------+------+----------+
// |    id|amount|ms3_amount|
// +------+------+----------+
// |tester|     1|         4|
// |tester|     3|        14|
// |tester|    10|        13|
// |curycu|    10|        13|
// |curycu|     3|        20|
// |curycu|     7|        12|
// |curycu|     2|         9|
// +------+------+----------+


//// 누적합 계산
val wcumsum = Window.partitionBy("id").orderBy("date").rowsBetween(Window.unboundedPreceding, Window.currentRow)
// 위와 동일한데 범위가 [시작점, 현재 로우] 이다

df.withColumn("cumsum_amount", sum($"amount").over(wcumsum))
  .select("id", "amount", "cumsum_amount")
  .show()
// +------+------+-------------+
// |    id|amount|cumsum_amount|
// +------+------+-------------+
// |tester|     1|            1|
// |tester|     3|            4|
// |tester|    10|           14|
// |curycu|    10|           10|
// |curycu|     3|           13|
// |curycu|     7|           20|
// |curycu|     2|           22|
// +------+------+-------------+
```



#### 7.4 그룹화 셋
- 여러 그룹테 걸쳐 집계할 수 있는 도구
- 집계 사이의 결합을 지원하는 저수준 기능
- SQL 문에서만 사용 가능
  - GROUP BY GROUPING SETS(그룹화단위1, 그룹화단위2, ...) 형태로 사용
  - 사실 각각 GROUP BY 한다음 UNION 한거라고 생각하면 됨
```sql
-- 대출 기간과 대출 종류를 각각 묶어서 결과 집계
SELECT period, gubun, SUM(loan_jan_amt) totl_jan
FROM kor_loan_status
WHERE period LIKE '2013%'
GROUP BY GROUPING SETS(period, gubun);
/*
    PERIOD   GUBUN                TOTL_JAN
    -------- -------------------- -------------------------
    201310                        1087493.9
    201311                        1095358.2
             기타대출               1357199.3
             주택담보대출            825652.8
*/


-- 대출 기간은 따로, 대출 종류와 지역은 함께 묶어서 결과 집계
SELECT period, gubun, region, SUM(loan_jan_amt) totl_jan
FROM kor_loan_status
WHERE period LIKE '2013%' AND region IN ('서울', '경기')
GROUP BY GROUPING SETS(period, (gubun, region));

/*
    PERIOD   GUBUN            REGION     TOTL_JAN
    -------- ---------------- ---------- ----------
             기타대출            서울       410563.1
             주택담보대출         서울       256484.5
             주택담보대출         경기       218357.4
             기타대출            경기       345934.5
    201310                               614460.4
    201311                               616879.1
*/
```

##### 7.4.1 롤업
- 그룹화 셋을 Dataset과 DataFrame에서 지원하기 위한 방법 중 하나
  - 데이터를 계층 단위로 표현하고 싶을때 유리
- 주어진 컬럼들의 모든 조합을 만들어내고 각 조합 단위로 그룹화 후 집계
  - 이때 전체에 대한 집계 결과도 각 컬럼 값을 NULL 로해서 추가됨
  - A 컬럼에 값이 5개, B 컬럼에 값이 3개 있다면 => 5 * 3 + 1 = 16개 조합 생성
- 사용법
  - df.rollup("컬럼1", "컬럼2", ... ).agg(집계함수)
  - 주의사항: rollup할 컬럼에 NULL값이 포함되어있다면 제대로 된 동작을 보장하지 못함
```scala
// 데이터를 1레벨 날짜, 2레벨 국가로 집계해서 보고자 하는경우
val rolledUpDf = dfNoNull.rollup("Date", "Country").agg(sum("Quantity")) // Date와 Country를 기준으로 롤업 후 각각 Quantity 합 집계
    .select("Date", "Country", "sum(Quantity)")                          // 컬럼과 집계한 값을 가져옴
    .orderBy("Date")
    
rolledUpDf.show()
/*
--------------------------------------------
Date       | Country        | sum(Quantity)
--------------------------------------------
null       | null           | 5176450
2010-12-01 | United Kingdom | 23949 
2010-12-01 | Germany        | 117
2010-12-01 | France         | 449
...
2010-12-03 | France         | 239
2010-12-03 | Italy          | 164
...
--------------------------------------------
*/
```

##### 7.4.2 큐브
- 모든 차원에 대해 고려하여 조합을 만들어 집계
- 롤업 결과 + 컬럼 하나씩만으로 집계된 결과
  - 롤업의 상위 버전이라 생각하면 됨
  - A 컬럼에 값이 5개, B 컬럼에 값이 3개 있다면 => (5 + 1) * (3 + 1) = 24개 조합 생성
  - 컬럼 값이 null이란 이야기는 해당 컬럼을 무시하고 집계했다고 생각하면 됨
- 사용법
  - df.cube("컬럼1", "컬럼2", ... ).agg(집계함수)
  - 주의사항: cube할 컬럼에 NULL값이 포함되어있다면 제대로 된 동작을 보장하지 못함
```scala
// 데이터를 날짜와 국가의 모든 집합으로 집계해서 보고자 하는경우
val cubeDf = dfNoNull.cube("Date", "Country").agg(sum("Quantity")) // Date와 Country를 기준으로 큐브 생성 후 각각 합 집계
    .select("Date", "Country", "sum(Quantity)")                    // 컬럼과 집계한 값을 가져옴
    .orderBy("Date")

cubeDf.show()
/*
--------------------------------------------
Date       | Country        | sum(Quantity)
--------------------------------------------
null       | null           | 5176450
null       | Japan          | 25218
null       | Portugal       | 16180
...
2010-12-01 | null           | ...
2010-12-01 | Japan          | ...
2010-12-01 | Portugal       | ...
...
2010-12-03 | null           | ...
2010-12-03 | Japan          | ...
2010-12-03 | Portugal       | ...
...
--------------------------------------------
*/
```


##### 7.4.3 그룹화 메타데이터
- 롤업이나 큐브 사용시 집계 레벨을 표현하는 형태
- 레벨 계산 법
  - 모든 필드의 값이 있는 경우의 조합이 0
  - 전체 집계가 2^n - 1
  - 레벨 예시 - 3개의 필드 (A, B, C)를 순서대로 조합할 경우
    - 0: A, B, C 모두 고려하여 집계
    - 1: B, C 만 고려하여 집계
    - 2: A, C 만 고려하여 집계
    - 3: A, B 만 고려하여 집계
    - 4: C 만 고려하여 집계
    - 5: B 만 고려하여 집계
    - 6: A 만 고려하여 집계
    - 7: 전체 집계
- grouping_id() 라는 집계 함수를 포함시켜 사용 가능
```scala
// 2개의 컬럼을 기준으로 집계하므로 [0, 3] 범위의 grouping_id를 가짐
val cubeDf = dfNoNull.cube("Date", "Country").agg(grouping_id(), sum("Quantity"))      // 아까에 grouping_id() 추가
    .orderBy(col("grouping_id()").desc)                                                // 그룹 레벨 기준으로 정렬

cubeDf.show()

/*
-----------------------------------------------------------
Date       | Country        | sum(Quantity) | grouping_id() 
-----------------------------------------------------------
null       | null           | 5176450       | 3
null       | Japan          | 25218         | 2
null       | Portugal       | 16180         | 2
...
2010-12-01 | null           | ...           | 1
2010-12-01 | Japan          | ...           | 0
2010-12-01 | Portugal       | ...           | 0
...
2010-12-03 | null           | ...           | 1
2010-12-03 | Japan          | ...           | 0
2010-12-03 | Portugal       | ...           | 0
...
--------------------------------------------
*/
```

##### 7.4.4 피벗
- 로우의 값을 컬럼으로 변환하여 집계하는 방법
  - 2차원 테이블을 만들어 집계하기 위한 방법
- 집계 후 자동으로 "피봇컬럼값_집계함수(나머지컬럼명)" 컬럼이 생성됨
- 사용법
  - df.groupBy(1차원컬럼명).pivot(피봇컬럼명).집계함수()
```scala
val pivoted = dfWithDate.groupBy("date").pivot("Country").sum()
pivoted.select("date", "`USA_sum(Quantity)`", "`Japan_sum(Quantity)`").show()

/*
-----------------------------------------------------
 date       | USA_sum(Quantity) | Japan_sum(Quantity)
 ----------------------------------------------------
 2011-12-06 | null              | 252
 2011-12-09 | 125               | 622
 2011-12-10 | null              | null
 2011-12-21 | 252               | null
 ...
*/
```


#### 7.5 사용자 정의 집계 함수
- 직접 집계함수를 만드는 방법
- UDAF라고 부름
- 생성법
  - UserDefinedAggregateFunction을 상속받아 생성
  - 메소드 정의 필요
    - inputSchema: 입력 파라미터 스키마를 StructType으로 정의
    - bufferSchema: 중간 결과 스키마를 StructType으로 정의
    - dataType: 반환될 값의 DataType 정의
    - deterministic:  입력값이 같으면 반환값이 같은지 여부를 boolean으로 정의
    - initialize: 집계용 버퍼의 값을 초기화하는 로직 정의
    - update: 입력받은 로우를 기바으로 내부 버퍼를 업데이트 하는 로직 정의
    - merge: 두개의 집계용 버퍼를 병합하는 로직 정의
    - evaluate: 집계의 최종 결과를 생성하는 로직 정의
- 사용법
  - 등록: spark.udf.register(사용할함수명, new 정의된객체)
  - 사용: select(expr("사용할함수명(필드명)"))
- 연산 수행되는 방법
  - 그룹단위로 해당 객체가 생성됨
  - 객체 내에는 mutable한 buffer가 있는데 이 버퍼의 초기화가 생성 단계에 이루어짐
    - initialize 함수에서 어떻게 초기화할지 정의
  - Row의 값이 하나씩 들어오면서 update 함수 내에서 buffer와 연산되어 buffer의 값이 계속 갱신됨
    - 연산 
  - 모든 Row에 대해 update 연산이 끝나면 evaluate 를 실행하고 그 결과 리턴
```scala
// 그룹 내 모든 값이 true인지 체크하는 BoolAnd 정의

class BoolAnd extends UserDefinedAggregateFunction {
    def inputSchema: StructType = StructType(StructField("value", BooleanType) :: Nil) // 시퀀스로 넣어줘야되서 ::Nil붙여서 시퀀스로
                                                                                       // 입력은 1개 boolean타입 컬럼
    
    def bufferedSchema: StructType = StructType(StructField("result", BooleanType) :: Nil) // 결과도 1개 boolean타입 컬럼
    
    def dataType: DataType = BooleanType // 값이 true인지 체크니깐 boolean 타입
    
    def deterministic: Boolean = True // 입력이 같으면 출력이 같은지 여부
    
    def initialize(buffer: MutableAggregationBuffer): Unit = {  // 연산하기 위한 초기값 지정 (&& 연산 할거니깐 true로)
        buffer(0) = true
    }
    
    def update(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = { // 연산시에는 기존 값과 새로운 값의 && 연산
        buffer1(0) = buffer1.getAs[Boolean](0) && buffer2.getAs[Boolean](0)
    }
    
    def evaluate(buffer: Row): Any = { // 결과 리턴 방법 정의
        buffer(0)
    }
}




```
