- 집계함수
  - 키나 그룹을 지정하고 하나 이상의 컬럼을 변환하는 방법 지정
  - 여러 입력값에 대해 그룹별로 결과 생성
  - 합, 곱, 카운팅, 평균 등
  
- 그룹화 데이터 타입 생성 방법
  - select 구문에서 직접 그룹화
  - group by: 가장 기본
  - window
  - grouping set: 서로 다른 레벨의 값 집계
  - roll up: 계층적인 그룹화
  - cube: 컬럼 조합에 대한 요약

- 그룹화 결과는 RelationalGroupedDataset 형태로 저장


####  7.1 집계 함수
- 모든 집계는 함수 사용
  - 예외는 있으나 무시할만한 수준이므로 패스
  - org.apache.spark.sql.functions 내에 존재
- 데이터 프레임레벨과 그룹 레벨에서 모두 사용 가능
  
##### 7.1.1 count
- 액션이 아닌 트랜스포메이션
  - 액션: 바로 결과를 출력한다 - 특정 값의 출력
  - 트랜스포메이션: 지연해서 결과를 출력한다 - 데이터의 변환
- 사용 방법
  - count(필드명) : 해당 필드값이 null 이 아닌 카운트 수 출력
  - count(1) : 필드값 상관없이 전체 카운트 출력

##### 7.1.2 countDistinct
- count와 동일하게 쓰지만 distinct된 수 출력

##### 7.1.3 approx_count_distinct
- 정확한 distinct count가 아닌 근사치 출력
- 사용방법
  - approx_count_distinct(필드명, 최대 추정 오류율)
  - 최대 추정 오류율에는 비율 넣으면 됨
    
##### 7.1.4 fisrt, last
- 맨 앞이나 마지막 값 출력
- 실제 저장된 row를 기반으로 동작

##### 7.1.5, 6, 7, 8 min, max, sum, sumDistinct, avg, mean
- 최소값, 최대값, 합, 중복 제거 합, 평균
- 다음과 같은 형태로도 집계 함수 사용 가능
```scala
df.select(
    count("quantity").alias("total_cnt"),         // 직접 집계 함수 사용
    sum("quantity").alias("total_sum"),
    avg("quantity").alias("total_avg"),
    expr("mean(quantity)").alias("total_mean"))   // expr로 집계 함수 사용 명령을 넘겨줌
  .selectExpr(
    "total_sum/total_cnt",                        // 연산이 있다는걸 seletExpr로 알려주고 연산 수행
    "total_avg",
    "total_mean")
```


##### 7.1.9 분산과 표준편차
- 모 분산, 표준편차: var_pop, stddev_pop
- 표본 분산, 표준편차: var_samp, stddev_samp

##### 7.1.10 비대칭도와 첨도
- 비대칭도 (skewnewss), 첨도 (kurtosis) 모두 데이터의 변곡점을 측정하는 방식
  - 비대칭도: 데이터 평균의 비대칭 정도 측정
  - 첨도: 데이터의 끝 부분 측정

##### 7.1.11 공분산과 상관관계
- 상관관계: corr(필드명1, 필드명2)
- 모공분산: covar_pop(필드명1, 필드명2)
- 표본공분산: covar_samp(필드명1, 필드명2)

##### 7.1.12 복합 데이터 타입의 집계
- 어떤 필드에 어떤 데이터가 있는지 확인해서 리스트로 변환
- collect_set(필드명): 중복을 제거해서 리스트로 변환
- collect_list(필드명): 중복을 허용해서 리스트로 변환
```scala
df.agg(collect_set("Country"), collect_list("Country")).show()
// collect_set(Country)           | collect_list(Country)
// [Portugal, Italy, France, ...] | [Portugal, Italy, Italy, ...]
```


#### 7.2 그룹화
- 데이터 그룹 단위의 집계를 수행하는 방법
- 두 단계로 나누어짐
  - 1. 그룹화 단계
    - 데이터를 기준에 따라 그룹화
    - RelationalGroupedDataset 형태로 리턴
  - 2. 집계 단계
    - 그룹화된 데이터에 집계함수를 적용
    - DataFrame 형태
```scala
df.groupBy("InvoiceNo", "CustomerId").count().show()
```

##### 7.2.1 표현식을 이용한 그룹화
- agg를 사용해서 표현식으로 그룹 데이터 집계 가능
```scala
df.groupBy("InvoiceNo").agg(
    count("Quantity").alias("quan"),
    expr("count(Quantity)"))
.show() // 둘다 같은값 출력
```

##### 7.2.2 맵을 이용한 그룹화
- 컬럼을 키, 수행할 집계 함수의 이름을 값으로 하는 map을 사용해서 그룹 트랜스포메이션 정의 가능
- 다양한 연산시나 연산 재활용시 용이
```scala
df.groupBy("InvoiceNo").agg("Quantity" -> "avg", "Quantity" -> "stddev_pop").show()

val temp = Map[String, String]("Quantity" -> "avg", "Quantity" -> "stddev_pop")  // 해보진 않았음... 대충 이런 의미
df.groupBy("InvoiceNo").agg(temp).show()
```


#### 7.3 윈도우 함수
- 데이터의 특정 "윈도우"에 대해 집계연산
- groupBy와의 차이점
  - groupBy
    - 그룹 내에 있는 데이터의 "통계"를 구한다
    - 리턴값은 그룹마다 하나씩이다
    - 범위는 항상 키로 나뉜 그룹 전체이다
    - 주 사용 범위: 전체 합, 전체 평균, Top 1, Bottom 1 등
  - window
    - 그룹 내에 있는 데이터끼리 "연산한 결과"를 가져온다
      - 물론 groupBy기능도 구성 가능
    - 리턴값은 모든 로우에 포함된다
    - 윈도우의 범위는 키로 나뉜 그룹 중 기준 row를 i라 할때 [i-x, i+y] 범위 내에 있는 값들로 구성된다
    - 주 사용 범위: Top N, moving average, row diff, 누적 합 등
- 사용방법
  - Window의 특성(Window Spec)을 먼저 정의한다
    - 파티션, 정렬, 데이터 범위 등
  - DataFrame.withColum("필드명", 집계함수.over(WindowSpec)) 형태로 호출한다
  - 이후 "필드명" 필드에 (집계함수 + WindowSpec)의 조합으로 연산된 값이 저장된다
```scala
import org.apache.spark.sql._
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window

val spark = SparkSession.builder().appName("sparksql").master("local").getOrCreate()
import spark.implicits._

val df =
  Seq(
    ("curycu", "2018-01-01", 10),
    ("curycu", "2018-01-07", 3),
    ("curycu", "2018-01-12", 7),
    ("curycu", "2018-01-15", 2),
    ("tester", "2018-01-01", 1),
    ("tester", "2018-01-11", 3),
    ("tester", "2018-01-18", 10))
    .toDF("id", "date", "amount")



//// Top 2 출력
val w = Window.partitionBy("id").orderBy("date")   // id를 키로 가지는 그룹에 그룹내에는 date를 기준으로 정렬하는 형태의 윈도우 정의

df.withColumn("rank", rank().over(w))   // rank 필드에는 w 에서 정의한 연산 (같은 id끼리 묶은 뒤 date 기준 정렬)을 적용하고 rank를 수행한다
  .filter(expr("rank <= 2"))
  .select("id", "date", "rank")
  .show()                               // rank가 1, 2인 ("tester", "2018-01-01", 1), ("tester", "2018-01-11", 2), 
                                        // ("curycu", "2018-01-01", 1), ("curycu", "2018-01-07", 2) 가 출력된다.


//// 부분합 계산
val wms3 = Window.partitionBy("id").orderBy("date").rowsBetween(-1, 1)  // id를 키로 가지는 그룹에 그룹내에는 date를 기준으로 정렬하며
                                                                        // 기준부터 위1, 아래1 까지 범위 3인 윈도우 정의

df.withColumn("ms3_amount", sum($"amount").over(wms3))  // ms3_amount 필드에는 wms3 에서 정의한 연산을 적용하고 3개씩 나누어 합을 구한다
  .select("id", "amount", "ms3_amount")
  .show()
// +------+------+----------+
// |    id|amount|ms3_amount|
// +------+------+----------+
// |tester|     1|         4|
// |tester|     3|        14|
// |tester|    10|        13|
// |curycu|    10|        13|
// |curycu|     3|        20|
// |curycu|     7|        12|
// |curycu|     2|         9|
// +------+------+----------+


//// 누적합 계산
val wcumsum = Window.partitionBy("id").orderBy("date").rowsBetween(Window.unboundedPreceding, Window.currentRow)
// 위와 동일한데 범위가 [시작점, 현재 로우] 이다

df.withColumn("cumsum_amount", sum($"amount").over(wcumsum))
  .select("id", "amount", "cumsum_amount")
  .show()
// +------+------+-------------+
// |    id|amount|cumsum_amount|
// +------+------+-------------+
// |tester|     1|            1|
// |tester|     3|            4|
// |tester|    10|           14|
// |curycu|    10|           10|
// |curycu|     3|           13|
// |curycu|     7|           20|
// |curycu|     2|           22|
// +------+------+-------------+
```



#### 7.4 그룹화 셋
- 여러 그룹테 걸쳐 집계할 수 있는 도구
- 집계 사이의 결합을 지원하는 저수준 기능
- SQL 문에서만 사용 가능
  - GROUP BY GROUPING SETS(그룹화단위1, 그룹화단위2, ...) 형태로 사용
  - 사실 각각 GROUP BY 한다음 UNION 한거라고 생각하면 됨
```sql
-- 대출 기간과 대출 종류를 각각 묶어서 결과 집계
SELECT period, gubun, SUM(loan_jan_amt) totl_jan
FROM kor_loan_status
WHERE period LIKE '2013%'
GROUP BY GROUPING SETS(period, gubun);
/*
    PERIOD   GUBUN                TOTL_JAN
    -------- -------------------- -------------------------
    201310                        1087493.9
    201311                        1095358.2
             기타대출               1357199.3
             주택담보대출            825652.8
*/


-- 대출 기간은 따로, 대출 종류와 지역은 함께 묶어서 결과 집계
SELECT period, gubun, region, SUM(loan_jan_amt) totl_jan
FROM kor_loan_status
WHERE period LIKE '2013%' AND region IN ('서울', '경기')
GROUP BY GROUPING SETS(period, (gubun, region));

/*
    PERIOD   GUBUN            REGION     TOTL_JAN
    -------- ---------------- ---------- ----------
             기타대출            서울       410563.1
             주택담보대출         서울       256484.5
             주택담보대출         경기       218357.4
             기타대출            경기       345934.5
    201310                               614460.4
    201311                               616879.1
*/
```

##### 7.4.1 롤업
- 그룹화 셋을 Dataset과 DataFrame에서 지원하기 위한 방법 중 하나
  - 데이터를 계층 단위로 표현하고 싶을때 유리
- 주어진 컬럼들의 모든 조합을 만들어내고 각 조합 단위로 그룹화 후 집계
  - 이때 전체에 대한 집계 결과도 각 컬럼 값을 NULL 로해서 추가됨
  - A 컬럼에 값이 5개, B 컬럼에 값이 3개 있다면 => 5 * 3 + 1 = 16개 조합 생성
- 사용법
  - df.rollup("컬럼1", "컬럼2", ... ).agg(집계함수)
  - 주의사항: rollup할 컬럼에 NULL값이 포함되어있다면 제대로 된 동작을 보장하지 못함
```scala
// 데이터를 1레벨 날짜, 2레벨 국가로 집계해서 보고자 하는경우
val rolledUpDf = dfNoNull.rollup("Date", "Country").agg(sum("Quantity")) // Date와 Country를 기준으로 롤업 후 각각 Quantity 합 집계
    .selectExpr("Date", "Country", "`sum(Quantity)` as total_quantity")  // sum(Quantity)의 연산이 다시 필요한지는 의문 추후 체크 필요
    .orderBy("Date")
    
rolledUpDf.show()
/*
--------------------------------------------
Date       | Country        | total_quantity
--------------------------------------------
null       | null           | 5176450
2010-12-01 | United Kingdom | 23949 
2010-12-01 | Germany        | 117
2010-12-01 | France         | 449
...
2010-12-03 | France         | 239
2010-12-03 | Italy          | 164
...
--------------------------------------------
*/
```

##### 7.4.2 큐브
- 모든 차원에 대해 고려하여 조합을 만들어 집계
- 롤업 결과 + 컬럼 하나씩만으로 집계된 결과
  - 롤업의 상위 버전이라 생각하면 됨
  - A 컬럼에 값이 5개, B 컬럼에 값이 3개 있다면 => (5 + 1) * (3 + 1) = 24개 조합 생성
  - 값이 null이란 
- 사용법
  - df.rollup("컬럼1", "컬럼2", ... ).agg(집계함수)
  - 주의사항: rollup할 컬럼에 NULL값이 포함되어있다면 제대로 된 동작을 보장하지 못함

