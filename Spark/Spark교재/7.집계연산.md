- 집계함수
  - 키나 그룹을 지정하고 하나 이상의 컬럼을 변환하는 방법 지정
  - 여러 입력값에 대해 그룹별로 결과 생성
  - 합, 곱, 카운팅, 평균 등
  
- 그룹화 데이터 타입 생성 방법
  - select 구문에서 직접 그룹화
  - group by: 가장 기본
  - window
  - grouping set: 서로 다른 레벨의 값 집계
  - roll up: 계층적인 그룹화
  - cube: 컬럼 조합에 대한 요약

- 그룹화 결과는 RelationalGroupedDataset 형태로 저장


####  7.1 집계 함수
- 모든 집계는 함수 사용
  - 예외는 있으나 무시할만한 수준이므로 패스
  - org.apache.spark.sql.functions 내에 존재
- 데이터 프레임레벨과 그룹 레벨에서 모두 사용 가능
  
##### 7.1.1 count
- 액션이 아닌 트랜스포메이션
  - 액션: 바로 결과를 출력한다 - 특정 값의 출력
  - 트랜스포메이션: 지연해서 결과를 출력한다 - 데이터의 변환
- 사용 방법
  - count(필드명) : 해당 필드값이 null 이 아닌 카운트 수 출력
  - count(1) : 필드값 상관없이 전체 카운트 출력

##### 7.1.2 countDistinct
- count와 동일하게 쓰지만 distinct된 수 출력

##### 7.1.3 approx_count_distinct
- 정확한 distinct count가 아닌 근사치 출력
- 사용방법
  - approx_count_distinct(필드명, 최대 추정 오류율)
  - 최대 추정 오류율에는 비율 넣으면 됨
    
##### 7.1.4 fisrt, last
- 맨 앞이나 마지막 값 출력
- 실제 저장된 row를 기반으로 동작

##### 7.1.5, 6, 7, 8 min, max, sum, sumDistinct, avg, mean
- 최소값, 최대값, 합, 중복 제거 합, 평균
- 다음과 같은 형태로도 집계 함수 사용 가능
```scala
df.select(
    count("quantity").alias("total_cnt"),         // 직접 집계 함수 사용
    sum("quantity").alias("total_sum"),
    avg("quantity").alias("total_avg"),
    expr("mean(quantity)").alias("total_mean"))   // expr로 집계 함수 사용 명령을 넘겨줌
  .selectExpr(
    "total_sum/total_cnt",                        // 연산이 있다는걸 seletExpr로 알려주고 연산 수행
    "total_avg",
    "total_mean")
```


##### 7.1.9 분산과 표준편차
- 모 분산, 표준편차: var_pop, stddev_pop
- 표본 분산, 표준편차: var_samp, stddev_samp

##### 7.1.10 비대칭도와 첨도
- 비대칭도 (skewnewss), 첨도 (kurtosis) 모두 데이터의 변곡점을 측정하는 방식
  - 비대칭도: 데이터 평균의 비대칭 정도 측정
  - 첨도: 데이터의 끝 부분 측정

##### 7.1.11 공분산과 상관관계
- 상관관계: corr(필드명1, 필드명2)
- 모공분산: covar_pop(필드명1, 필드명2)
- 표본공분산: covar_samp(필드명1, 필드명2)

##### 7.1.12 복합 데이터 타입의 집계
- 어떤 필드에 어떤 데이터가 있는지 확인해서 리스트로 변환
- collect_set(필드명): 중복을 제거해서 리스트로 변환
- collect_list(필드명): 중복을 허용해서 리스트로 변환
```scala
df.agg(collect_set("Country"), collect_list("Country")).show()
// collect_set(Country)           | collect_list(Country)
// [Portugal, Italy, France, ...] | [Portugal, Italy, Italy, ...]
```


#### 7.2 그룹화
- 데이터 그룹 단위의 집계를 수행하는 방법
- 두 단계로 나누어짐
  - 1. 그룹화 단계
    - 데이터를 기준에 따라 그룹화
    - RelationalGroupedDataset 형태로 리턴
  - 2. 집계 단계
    - 그룹화된 데이터에 집계함수를 적용
    - DataFrame 형태
```scala
df.groupBy("InvoiceNo", "CustomerId").count().show()
```

##### 7.2.1 표현식을 이용한 그룹화
- agg를 사용해서 표현식으로 그룹 데이터 집계 가능
```scala
df.groupBy("InvoiceNo").agg(
    count("Quantity").alias("quan"),
    expr("count(Quantity)"))
.show() // 둘다 같은값 출력
```

##### 7.2.2 맵을 이용한 그룹화
- 컬럼을 키, 수행할 집계 함수의 이름을 값으로 하는 map을 사용해서 그룹 트랜스포메이션 정의 가능
- 다양한 연산시나 연산 재활용시 용이
```scala
df.groupBy("InvoiceNo").agg("Quantity" -> "avg", "Quantity" -> "stddev_pop").show()

val temp = Map[String, String]("Quantity" -> "avg", "Quantity" -> "stddev_pop")  // 해보진 않았음... 대충 이런 의미
df.groupBy("InvoiceNo").agg(temp).show()
```


##### 7.2.3 윈도우 함수
- 데이터의 특정 "윈도우"에 대해 집계연산
- groupBy와의 차이점
  - 사이즈가 







- cor, corr


##### 7.1.6 sum
