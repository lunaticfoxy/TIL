### 자주 사용되는 피쳐들을 생성하는 방법 정리 예정

#### TF-IDF
```scala
  val allcommentsDf = spark.sql("select * from table")    // 데이터 로드

  val tokenizer = new Tokenizer().setInputCol("content").setOutputCol("words")
  val wordsData = tokenizer.transform(allcommentsDf)      // 토큰단위로 분리 (입력: content, 출력: words)

  val hashingTF = new HashingTF().setInputCol("words").setOutputCol("rawFeatures") //.setNumFeatures(20)
  val featurizedData = hashingTF.transform(wordsData)     // TF 벡터 생성 (입력: words, 출력: rawFeatures)

  val idf = new IDF().setInputCol("rawFeatures").setOutputCol("features")
  val idfModel = idf.fit(featurizedData)                  // IDF 계산
  val rescaledData = idfModel.transform(featurizedData)   // TF벡터를 TF-IDF 벡터로 변환 (입력: rawFeatures, 출력: features)
  
  val features = rescaledData.select("id", "content", "features")
  features.write.mode("overwrite").saveAsTable("res_table") // 테이블에 결과 저장 (features는 Sparse Vector 형태로 저장됨)
```

#### TF 벡터 (Counter 벡터)
```scala
val vocabSize = 100000

val cvModel = new CountVectorizer()
    .setInputCol("words")
    .setOutputCol("features")

val cvModelFit = cvModel.setVocabSize(vocabSize).fit(wordsData) // 최대 vocabSize 만큼 벡터 구성

val featurizedData = cvModelFit.transform(wordsData)             // 벡터 생성
```

#### Onehot vector
```scala
val df = spark.createDataFrame(
Seq((0, "Jason", "Germany"),
(1, "David", "France"),
(2, "Martin", "Spain"),
(3, "Jason", "USA"),
(4, "Daiel", "UK"),
(5, "Moahmed", "Bangladesh"),
(6, "David", "Ireland"),
(7, "Jason", "Netherlands"))).toDF("id", "name", "address")


import org.apache.spark.ml.feature.StringIndexer
import org.apache.spark.ml.feature.OneHotEncoder


val indexer = new StringIndexer()                       // StringIndexer를 통해 단어별 index 생성
                 .setInputCol("name")
                 .setOutputCol("categoryIndex")
                 .fit(df)

val indexed = indexer.transform(df)                     // 데이터의 단어를 모두 index로 변경

val encoder = new OneHotEncoder()                       // 인덱스를 기반으로 one-hot 벡터를 만드는 endoder 생성
                 .setInputCol("categoryIndex")
                 .setOutputCol("categoryVec")

val encoded = encoder.transform(indexed)
encoded.show()
```


#### Word2Vec
```scala
import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel} 

val input = sc.textFile("data/mllib/sample_lda_data.txt")
              .map(line => line.split(" ").toSeq) 
val word2vec = new Word2Vec() 
val model = word2vec.fit(input) 
val synonyms = model.findSynonyms("1", 5) 

for((synonym, cosineSimilarity) <- synonyms) { 
    println(s"$synonym $cosineSimilarity") 
} 

// Save and load model 
model.save(sc, "myModelPath") 
val sameModel = Word2VecModel.load(sc, "myModelPath")
```


#### LSH - MinHash
```scala 
val dfA = spark.createDataFrame(Seq(
  (0, Vectors.sparse(6, Seq((0, 1.0), (1, 1.0), (2, 1.0)))),
  (1, Vectors.sparse(6, Seq((2, 1.0), (3, 1.0), (4, 1.0)))),
  (2, Vectors.sparse(6, Seq((0, 1.0), (2, 1.0), (4, 1.0))))
)).toDF("id", "features")

val dfB = spark.createDataFrame(Seq(
  (3, Vectors.sparse(6, Seq((1, 1.0), (3, 1.0), (5, 1.0)))),
  (4, Vectors.sparse(6, Seq((2, 1.0), (3, 1.0), (5, 1.0)))),
  (5, Vectors.sparse(6, Seq((1, 1.0), (2, 1.0), (4, 1.0))))
)).toDF("id", "features")

val key = Vectors.sparse(6, Seq((1, 1.0), (3, 1.0)))

val mh = new MinHashLSH()
  .setNumHashTables(5)
  .setInputCol("features")
  .setOutputCol("hashes")

val model = mh.fit(dfA)

// Feature Transformation
println("The hashed dataset where hashed values are stored in the column 'hashes':")
model.transform(dfA).show()

// Compute the locality sensitive hashes for the input rows, then perform approximate
// similarity join.
// We could avoid computing hashes by passing in the already-transformed dataset, e.g.
// `model.approxSimilarityJoin(transformedA, transformedB, 0.6)`
println("Approximately joining dfA and dfB on Jaccard distance smaller than 0.6:")
model.approxSimilarityJoin(dfA, dfB, 0.6, "JaccardDistance")
  .select(col("datasetA.id").alias("idA"),
    col("datasetB.id").alias("idB"),
    col("JaccardDistance")).show()

// Compute the locality sensitive hashes for the input rows, then perform approximate nearest
// neighbor search.
// We could avoid computing hashes by passing in the already-transformed dataset, e.g.
// `model.approxNearestNeighbors(transformedA, key, 2)`
// It may return less than 2 rows when not enough approximate near-neighbor candidates are
// found.
println("Approximately searching dfA for 2 nearest neighbors of the key:")
model.approxNearestNeighbors(dfA, key, 2).show()
```
