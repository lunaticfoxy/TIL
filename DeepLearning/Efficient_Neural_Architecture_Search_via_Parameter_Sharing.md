논문 제목: Efficient Neural Architecture Search via Parameter Sharing

목적: ImageNet 분류 모델의 자동 구성 모델 개선

기본 개념
- 강화학습을 통해 모델의 구성을 학습시키는 모델 구성
- 기존 모델을 weight sharing을 통해 성능 개선


기존 연구
- 컨트롤러를 RNN으로 구성
- 두가지 구성 방법 사용
  - 한 레이어 내 구조를 마음대로 결정
    - 한 RNN 노드가 한 레이어의 피쳐를 하나씩 결정
    - Layer의 개수는 사람이 고정
    - 한 레이어 내의 구조만 결정
  - Normal Reduction 구조를 정해 학습
    - Normal Cell과 Reduction Cell을 나누어 구성
      - Normal Cell n개, Reduction Cell 1개의 구조를 반복
      - n은 사람이 결정, Cell 내부만 모델이 결정
    - Cell내 구성 방법
      - 레이어 1 구성
        - 레이어 인풋 지정 (RNN 블록 1)
        - 레이어 연산 지정 (RNN 블록 2)
      - 레이어 2 구성
        - 레이어 인풋 지정 (RNN 블록 3)
        - 레이어 연산 지정 (RNN 블록 4)
      - 두 레이어 합치는 방법 구성 (RNN 블록 5)
      
- 동기
  - NASsms 800GPU를 28일동안 썼고, NASNetdms 450 GPU를 3,4일간 썼다.
  - 병목이 어디냐 => 각 자식 모델 학습 시간
  - 그동안 학습시킨 자식 모델은? 어디가냐? 다 버리는건가? 이걸 써볼까?
  
- 컨트리뷰션
  - 자식 모델의 weight를 공유하자
  - 이론상 자식 모델을 한번만 학습시켜도 워킹
  - 요약 결과: 1080Ti 1대로 하루만에 학습! => 1000배 빨라짐
  
- 내용
  - DAG (일방향 무사이클 그래프)를 만드는 문제로 생각한다.
    - DAG에서 앞에 있는 노드부터 뒷 노드로 연결되는 엣지를 선택하는 문제로 생각
    - 한 노드씩 이를 반복하면 사이클 없이 마지막 노드로 도달 가능
    - 단 노드 개수는 사전 지정되어야 함
  - 예시 : Recurrent Cell을 새로 구성
    - LSTM, GRU 는 결국 사람이 구성한거니 한계 존재
    - 이거 구성도 모델한테 맡겨보자
    - 구성 방법
      - 첫번째 RNN 출력: 1번 노드의 연산 출력
      - 두번째 RNN 출력: 2번 노드의 입력 노드 출력
      - 세번째 RNN 출력: 2번 노드의 연산 출력
      - 네번째 RNN 출력: 3번 노드의 입력 노드 출력
      - 다섯번째 RNN 출력: 2번, 3번 노드의 연결 방법 출력
      - ...
  - 학습 방법
    - 각 노드들의 weight는 쉐어링한다
      - DAG의 노드 구성요소는 동일하니 모든 노드가 1:1 매핑됨
    - 컨트롤러인 rnn을 학습하며 진행
    - 강화학습 사용
      - 학습된 모델의 정확도를 reward로 사용
      
      
- 결과
  - LSTM
    - 당시 제안된 LSTM들이랑 거의 유사한 성능 탐색함
  - CNN
    - 아직 최고 성능은 안나온다...
    
- 의의?
  - 자동으로 모델 구성하는 문제에서 속도를 향상시킬수 있음
  - 아직 실제로 적용할때는 파라미터에 엄청 민감함
  - 그냥 가능하다 정도를 보여준 논문으로 생각됨
  
  
  
  
  
  
  
  
  
  
