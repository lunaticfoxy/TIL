#### Transformer XL을 활용해서 BERT보다 성능을 올린 XLNet

#### 주소: 

#### 내용
- Permutation Language Modeling
  - 사실 BERT의 방법은 근처 데이터를 활용해서 빈칸을 메꾸는 것과 같다
  - 그럼 모든 경우의 수를 따져서 MASK를 씌우면 성능이 오르지 않을까?
    - 연산량이 너무 많겠지
    - 그럼 연산량을 줄이기 위해 방법을 만들어보자!
  - [1,2,3,4,5] 에서 3을 Mask 취했을때 원하는건
    - P(3|1,2,4,5), P(3|1,2,4), P(3|1,2,5), P(3|2,4,5), P(3|1,2) ... 등이 있겠지
    - 하지만 BERT에서는 P(3|1,2,4,5) 밖에 고려하지 못한다
    - 이걸 Permutation을 취한다음
      - [1,3,2,4,5] => P(3|1)
      - [1,2,3,4,5] => P(3|1,2)
      - [1,2,4,3,5] => P(3|1,2,4)
    - 식으로 경우의 수를 만들어서 MASK 대상까지만 연산하면 연산량을 줄일 수 있따
    - 실제 연산량을 더 줄여야겠지만 이는 확인하지 못함
- Two Stream Self-Attention
  - 원래는 위치정보, 컨텍스트 정보가 모두 h에 들어가지만 이게 섞이면서 한쪽이 희석될 수 있다
    - 보통 컨텍스트 정보가 희석되니깐 아래 구조를 지닐듯
  - 위치 정보를 살리기 위해 새로 g라는 레이어를 h옆에 넣는다
  - 초기값에 g에는 위치정보만, h에는 컨텍스트 정보만 넣는다
  - 먼저 h 계산은 기존과 동일하게 한다
    - h에는 컨텍스트 정보만 저장
  - 이후 g 계산을 Q를 g에서 가져오고, K, V는 h에서 가져와서 연산한다
    - g에는 컨텍스트 정보와 위치 정보가 섞여서 저장
  - 이 과정을 한 단계로 수행한다
  - 최종결과는 g의 결과를 활용
